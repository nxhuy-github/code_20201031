{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn how to use \n",
    "- `tf.data.TextLineDataset`\n",
    "- `tfds.features.text.Tokenizer`\n",
    "- `tfds.features.text.TokenTextEncoder` + `tf.py_function` + `map`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRECTORY_URL = 'https://storage.googleapis.com/download.tensorflow.org/data/illiad/'\n",
    "FILE_NAMES = ['cowper.txt', 'derby.txt', 'butler.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/nxhuy/.keras/datasets'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for name in FILE_NAMES:\n",
    "    text_dir = tf.keras.utils.get_file(name, origin=DIRECTORY_URL+name)\n",
    "    \n",
    "parent_dir = os.path.dirname(text_dir)\n",
    "\n",
    "parent_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load text into datasets\n",
    "\n",
    "Iterate through the files, loading each one into its own dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labeler(example, index):\n",
    "    return example, tf.cast(index, tf.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `tf.data.TextLineDataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data_sets = []\n",
    "\n",
    "for i, file_name in enumerate(FILE_NAMES):\n",
    "    lines_dataset = tf.data.TextLineDataset(os.path.join(parent_dir, file_name))\n",
    "    labeled_dataset = lines_dataset.map(lambda ex: labeler(ex, i))\n",
    "    labeled_data_sets.append(labeled_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"\\xef\\xbb\\xbfAchilles sing, O Goddess! Peleus' son;\"\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for text, label in labeled_data_sets[0].take(1):\n",
    "    print(text.numpy())\n",
    "    print(label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 50000\n",
    "BATCH_SIZE = 64\n",
    "TAKE_SIZE = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labeled_data = labeled_data_sets[0]\n",
    "for labeled_data in labeled_data_sets[1:]:\n",
    "    all_labeled_data = all_labeled_data.concatenate(labeled_data)\n",
    "    \n",
    "all_labeled_data = all_labeled_data.shuffle(BUFFER_SIZE, reshuffle_each_iteration=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'Disabled sank; he fell supine, and bore'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'Shall want performance. But Olympian Jove!'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'One with hot current flows, and from beneath,'>, <tf.Tensor: shape=(), dtype=int64, numpy=1>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b'as of a great multitude.'>, <tf.Tensor: shape=(), dtype=int64, numpy=2>)\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b\"The wall destroy'd, o'er all the shore he spread\">, <tf.Tensor: shape=(), dtype=int64, numpy=1>)\n"
     ]
    }
   ],
   "source": [
    "for ex in all_labeled_data.take(5):\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode text lines as numbers\n",
    "\n",
    "Machine learning models **work on numbers, not words**, so the string values need to be converted into lists of numbers. To do that, map each unique word to a unique integer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build vocabulary\n",
    "\n",
    "Steps:\n",
    "- Iterate over each example's `numpy` value.\n",
    "- Use `tfds.features.text.Tokenizer` to split it into tokens.\n",
    "- Collect these tokens into a Python set, to remove duplicates.\n",
    "- Get the size of the vocabulary for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17178"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = tfds.features.text.Tokenizer()\n",
    "\n",
    "vocabulary_set = set()\n",
    "for text_sensor, _ in all_labeled_data:\n",
    "    some_tokens = tokenizer.tokenize(text_sensor.numpy())\n",
    "    vocabulary_set.update(some_tokens)\n",
    "    \n",
    "len(vocabulary_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocabulary_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode examples\n",
    "\n",
    "Using `tfds.features.text.TokenTextEncoder` + `tf.py_function` + `map`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = tfds.features.text.TokenTextEncoder(vocabulary_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Disabled sank; he fell supine, and bore'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_text = next(iter(all_labeled_data))[0].numpy()\n",
    "example_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3237, 3350, 1726, 15680, 1310, 9692, 12743]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text = encoder.encode(example_text)\n",
    "encoded_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    Now run the <b>encoder on the dataset</b> by wrapping it in <code>tf.py_function</code> and passing that to the dataset's <code>map</code> method.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3237, 3350, 1726, 15680, 1310, 9692, 12743]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.encode(b'Disabled sank; he fell supine, and bore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_encode(txt, label):\n",
    "    et = encoder.encode(txt.numpy())\n",
    "    return et, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3237, 3350, 1726, 15680, 1310, 9692, 12743] tf.Tensor(0, shape=(), dtype=int64)\n",
      "[17148, 4449, 2114, 6000, 10949, 5184] tf.Tensor(0, shape=(), dtype=int64)\n",
      "[1070, 11464, 15133, 707, 1740, 9692, 10946, 1226] tf.Tensor(1, shape=(), dtype=int64)\n",
      "[13985, 1143, 7681, 6488, 2804] tf.Tensor(2, shape=(), dtype=int64)\n",
      "[2280, 16991, 7626, 6972, 8621, 4823, 13780, 15824, 6577, 1726, 846] tf.Tensor(1, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# Test encode()\n",
    "for t, l in all_labeled_data.take(5):\n",
    "    e, _l = my_encode(t, l)\n",
    "    print(e, _l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You want to use `Dataset.map` to apply this function to each element of the dataset. `Dataset.map` runs in graph mode.\n",
    "\n",
    "- Graph tensors do not have a value.\n",
    "- In graph mode you can only use TensorFlow Ops and functions.\n",
    "\n",
    "\n",
    "So we **can't `.map`** this function **directly**: We need to wrap it in a `tf.py_function`. The `tf.py_function` will pass regular tensors (with a value and a `.numpy()` method to access it), to the wrapped python function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_map_fn(text, label):\n",
    "    # py_func DOESN'T set the shape of the returned tensors.\n",
    "    encoded_text, label = tf.py_function(my_encode, \n",
    "                                         inp=[text, label], \n",
    "                                         Tout=(tf.int64, tf.int64)\n",
    "                                        )\n",
    "    # `tf.data.Datasets` work best if all components have a shape set\n",
    "    #  so set the shapes manually:\n",
    "    encoded_text.set_shape([None])\n",
    "    label.set_shape([])\n",
    "    \n",
    "    return encoded_text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_encoded_data = all_labeled_data.map(encode_map_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(7,), dtype=int64, numpy=array([ 3237,  3350,  1726, 15680,  1310,  9692, 12743])>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      "(<tf.Tensor: shape=(6,), dtype=int64, numpy=array([17148,  4449,  2114,  6000, 10949,  5184])>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n",
      "(<tf.Tensor: shape=(8,), dtype=int64, numpy=array([ 1070, 11464, 15133,   707,  1740,  9692, 10946,  1226])>, <tf.Tensor: shape=(), dtype=int64, numpy=1>)\n",
      "(<tf.Tensor: shape=(5,), dtype=int64, numpy=array([13985,  1143,  7681,  6488,  2804])>, <tf.Tensor: shape=(), dtype=int64, numpy=2>)\n",
      "(<tf.Tensor: shape=(11,), dtype=int64, numpy=\n",
      "array([ 2280, 16991,  7626,  6972,  8621,  4823, 13780, 15824,  6577,\n",
      "        1726,   846])>, <tf.Tensor: shape=(), dtype=int64, numpy=1>)\n"
     ]
    }
   ],
   "source": [
    "for ex in all_encoded_data.take(5):\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the dataset into test and train batches\n",
    "\n",
    "Use `tf.data.Dataset.take` and `tf.data.Dataset.skip` to create a small test dataset and a larger training set.\n",
    "\n",
    "Before being passed into the model, the datasets need to be batched. Typically, **the examples inside of a batch need to be the same size and shape**. But, the examples in these datasets are not all the same size â€” each line of text had a different number of words. So use `tf.data.Dataset.padded_batch` (instead of `batch`) to pad the examples to the same size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = all_encoded_data.skip(TAKE_SIZE).shuffle(BUFFER_SIZE)\n",
    "train_data = train_data.padded_batch(BATCH_SIZE)\n",
    "\n",
    "test_data = all_encoded_data.take(TAKE_SIZE)\n",
    "test_data = test_data.padded_batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 14) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 17) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 17) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 14) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 17) (64,)\n",
      "(64, 17) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 18) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 18) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 14) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 17) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 17) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 17) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 14) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 14) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 17) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 14) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 14) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 14) (64,)\n",
      "(64, 14) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 17) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 14) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 14) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 14) (64,)\n",
      "(64, 17) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 13) (64,)\n",
      "(64, 14) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 14) (64,)\n",
      "(64, 17) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 18) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 17) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 14) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 17) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 14) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 14) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 17) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 17) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 14) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 14) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 14) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 17) (64,)\n",
      "(64, 14) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 14) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 14) (64,)\n",
      "(64, 17) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 17) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 14) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 17) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 17) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 17) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 19) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 17) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 17) (64,)\n",
      "(64, 14) (64,)\n",
      "(64, 17) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 18) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 17) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 17) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 14) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 14) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 17) (64,)\n",
      "(64, 14) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 17) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 18) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 18) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 17) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 17) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 18) (64,)\n",
      "(64, 17) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 17) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 17) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 18) (64,)\n",
      "(64, 18) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 17) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 17) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 17) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 14) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 17) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 17) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 14) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 13) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 17) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 17) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 17) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 17) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 14) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 14) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 18) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 17) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 17) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 14) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 17) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 14) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 14) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 17) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 17) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 17) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 17) (64,)\n",
      "(64, 14) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 14) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 17) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 14) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 14) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 17) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 17) (64,)\n",
      "(64, 17) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 17) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 17) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 14) (64,)\n",
      "(64, 17) (64,)\n",
      "(64, 17) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 17) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 17) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 17) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 17) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 14) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 17) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 17) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 17) (64,)\n",
      "(64, 14) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 18) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 18) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 17) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 17) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 17) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 16) (64,)\n",
      "(64, 15) (64,)\n",
      "(64, 18) (64,)\n"
     ]
    }
   ],
   "source": [
    "for ex in train_data:\n",
    "    print(ex[0].shape, ex[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(15,), dtype=int64, numpy=\n",
       " array([ 3237,  3350,  1726, 15680,  1310,  9692, 12743,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0])>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text, sample_labels = next(iter(test_data))\n",
    "\n",
    "sample_text[0], sample_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have introduced a **new token encoding (the zero used for padding)**, the vocabulary size has increased by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, 64),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(3)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 64)          1099456   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 128)               66048     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 1,178,115\n",
      "Trainable params: 1,178,115\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "697/697 [==============================] - 31s 44ms/step - loss: 0.5216 - accuracy: 0.7463 - val_loss: 0.3815 - val_accuracy: 0.8260\n",
      "Epoch 2/3\n",
      "697/697 [==============================] - 28s 40ms/step - loss: 0.2990 - accuracy: 0.8692 - val_loss: 0.3574 - val_accuracy: 0.8358\n",
      "Epoch 3/3\n",
      "697/697 [==============================] - 27s 38ms/step - loss: 0.2238 - accuracy: 0.9031 - val_loss: 0.3705 - val_accuracy: 0.8386\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f163a149fd0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data, epochs=3, validation_data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 3s 41ms/step - loss: 0.3705 - accuracy: 0.8386\n",
      "\n",
      "Eval loss: 0.371, Eval accuracy: 0.839\n"
     ]
    }
   ],
   "source": [
    "eval_loss, eval_acc = model.evaluate(test_data)\n",
    "print('\\nEval loss: {:.3f}, Eval accuracy: {:.3f}'.format(eval_loss, eval_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- https://www.tensorflow.org/tutorials/load_data/text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
