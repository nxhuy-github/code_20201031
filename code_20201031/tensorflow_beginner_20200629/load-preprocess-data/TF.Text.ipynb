{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "**Tensorflow Text** provides a collection of text related classes and [operations (ops for short)]() ready to use with TensorFlow 2.0. The library can perform the **preprocessing regularly required by text-based models**, and includes other features useful for sequence modeling not provided by core TensorFlow.\n",
    "\n",
    "The benefit of using these ops in your text preprocessing is that they are done in the TensorFlow graph. We do not need to worry about tokenization in training being different than the tokenization at inference, or managing preprocessing scripts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eager execution\n",
    "\n",
    "TensorFlow Text requires TensorFlow 2.0, and is fully compatible with eager mode and graph mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q tensorflow-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_text as text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unicode\n",
    "\n",
    "Most ops expect that the strings are in UTF-8. If we're using a different encoding, we can use the core tensorflow **transcode op** to transcode into UTF-8. We can also use the same op to coerce your string to structurally valid UTF-8 if our input could be invalid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=string, numpy=\n",
       "array([b'\\x00E\\x00v\\x00e\\x00r\\x00y\\x00t\\x00h\\x00i\\x00n\\x00g\\x00 \\x00n\\x00o\\x00t\\x00 \\x00s\\x00a\\x00v\\x00e\\x00d\\x00 \\x00w\\x00i\\x00l\\x00l\\x00 \\x00b\\x00e\\x00 \\x00l\\x00o\\x00s\\x00t\\x00.',\n",
       "       b'\\x00S\\x00a\\x00d&9'], dtype=object)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = tf.constant([\n",
    "    'Everything not saved will be lost.'.encode('UTF-16-BE'),\n",
    "    'Sad☹'.encode('UTF-16-BE')\n",
    "])\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=string, numpy=\n",
       "array([b'Everything not saved will be lost.', b'Sad\\xe2\\x98\\xb9'],\n",
       "      dtype=object)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utf8_docs = tf.strings.unicode_transcode(\n",
    "    docs,\n",
    "    input_encoding='UTF-16-BE',\n",
    "    output_encoding='UTF-8'\n",
    ")\n",
    "utf8_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Tokenization is the process of **breaking up a string into tokens**. Commonly, these tokens are words, numbers, and/or punctuation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WhitespaceTokenizer\n",
    "\n",
    "This is a basic tokenizer that splits UTF-8 strings on ICU defined whitespace characters (eg. space, tab, new line)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[b'everything', b'not', b'saved', b'will', b'be', b'lost.'],\n",
       " [b'Sad\\xe2\\x98\\xb9']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = text.WhitespaceTokenizer()\n",
    "tokens = tokenizer.tokenize(['everything not saved will be lost.', 'Sad☹'.encode('UTF-8')])\n",
    "tokens.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UnicodeScriptTokenizer\n",
    "\n",
    "This tokenizer splits UTF-8 strings based on Unicode script boundaries. The script codes used correspond to International Components for Unicode (ICU) UScriptCode values. \n",
    "\n",
    "In practice, this is similar to the `WhitespaceTokenizer` with the most apparent **difference** being that it will **split punctuation** (USCRIPT_COMMON) from language texts (eg. USCRIPT_LATIN, USCRIPT_CYRILLIC, etc) while also **separating language texts from each other**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[b'everything', b'not', b'saved', b'will', b'be', b'lost', b'.'],\n",
       " [b'Sad', b'\\xe2\\x98\\xb9']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = text.UnicodeScriptTokenizer()\n",
    "tokens = tokenizer.tokenize(['everything not saved will be lost.', 'Sad☹'.encode('UTF-8')])\n",
    "tokens.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unicode split\n",
    "\n",
    "When tokenizing languages without whitespace to segment words, it is common to just split by character, which can be accomplished using the `unicode_split` op found in core."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[b'\\xe4\\xbb\\x85', b'\\xe4\\xbb\\x8a', b'\\xe5\\xb9\\xb4', b'\\xe5\\x89\\x8d']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tf.strings.unicode_split([\"仅今年前\".encode('UTF-8')], 'UTF-8')\n",
    "tokens.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offset\n",
    "\n",
    "When tokenizing strings, it is often desired to know where in the original string the token originated from. For this reason, each tokenizer which implements `TokenizerWithOffsets` has a `tokenize_with_offsets` method that will **return the byte offsets along with the tokens**. \n",
    "- The `offset_starts` lists the bytes in the original string each token starts at.\n",
    "- The `offset_limits` lists the bytes where each token ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.RaggedTensor [[b'everything', b'not', b'saved', b'will', b'be', b'lost', b'.'], [b'Sad', b'\\xe2\\x98\\xb9']]>,\n",
       " <tf.RaggedTensor [[0, 11, 15, 21, 26, 29, 33], [0, 3]]>,\n",
       " <tf.RaggedTensor [[10, 14, 20, 25, 28, 33, 34], [3, 6]]>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = text.UnicodeScriptTokenizer()\n",
    "(tokens, offset_starts, offset_limits) = tokenizer.tokenize_with_offsets(\n",
    "    ['everything not saved will be lost.', 'Sad☹'.encode('UTF-8')]\n",
    ")\n",
    "tokens, offset_starts, offset_limits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF.Data example\n",
    "\n",
    "Tokenizers work as expected with the tf.data API. A simple example is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[b'Never', b'tell', b'me', b'the', b'odds.']], [[b\"It's\", b'a', b'trap!']])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = tf.data.Dataset.from_tensor_slices([['Never tell me the odds.'], [\"It's a trap!\"]])\n",
    "tokenizer = text.WhitespaceTokenizer()\n",
    "tokenized_docs = docs.map(lambda x: tokenizer.tokenize(x))\n",
    "iterator = iter(tokenized_docs)\n",
    "\n",
    "next(iterator).to_list(), next(iterator).to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Text Ops\n",
    "TF.Text packages other useful preprocessing ops. We will review a couple below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordshape\n",
    "\n",
    "A common feature used in some natural language understanding models is to see if the text string has a certain property. For example, a sentence breaking model might contain features which check for word capitalization or if a punctuation character is at the end of a string.\n",
    "\n",
    "**Wordshape** defines a variety of **useful regular expression** based helper functions for **matching various relevant patterns** in our input text. Here are a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[b'Everything', b'not', b'saved', b'will', b'be', b'lost.'],\n",
       " [b'Sad\\xe2\\x98\\xb9']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = text.WhitespaceTokenizer()\n",
    "tokens = tokenizer.tokenize(['Everything not saved will be lost.', 'Sad☹'.encode('UTF-8')])\n",
    "tokens.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[True, False, False, False, False, False], [True]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Is capitalized?\n",
    "f1 = text.wordshape(tokens, text.WordShape.HAS_TITLE_CASE)\n",
    "f1.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[False, False, False, False, False, False], [False]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Are all letters uppercased?\n",
    "f2 = text.wordshape(tokens, text.WordShape.IS_UPPERCASE)\n",
    "f2.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[False, False, False, False, False, True], [True]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Does the token contain punctuation?\n",
    "f3 = text.wordshape(tokens, text.WordShape.HAS_SOME_PUNCT_OR_SYMBOL)\n",
    "f3.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[False, False, False, False, False, False], [False]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Is the token a number?\n",
    "f4 = text.wordshape(tokens, text.WordShape.IS_NUMERIC_VALUE)\n",
    "f4.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gram & Sliding Window\n",
    "\n",
    "**N-grams are sequential words given a sliding window size of n**. When combining the tokens, there are three reduction mechanisms supported.\n",
    "- Text\n",
    "    - `Reduction.STRING_JOIN` which appends the strings to each other\n",
    "- Numerical values\n",
    "    - `Reduction.SUM`\n",
    "    - `Reduction.MEAN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[b'Everything', b'not', b'saved', b'will', b'be', b'lost.'],\n",
       " [b'Sad\\xe2\\x98\\xb9']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = text.WhitespaceTokenizer()\n",
    "tokens = tokenizer.tokenize(['Everything not saved will be lost.', 'Sad☹'.encode('UTF-8')])\n",
    "tokens.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[b'Everything not', b'not saved', b'saved will', b'will be', b'be lost.'], []]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams = text.ngrams(tokens, 2, reduction_type=text.Reduction.STRING_JOIN)\n",
    "bigrams.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References \n",
    "- https://www.tensorflow.org/tutorials/tensorflow_text/intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
